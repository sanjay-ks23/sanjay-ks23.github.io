---
layout: post
title:  "ML Summer School - Day 1"
author: Sanjay
categories: [Cohere]
image: "![image](https://github.com/user-attachments/assets/f1be48d3-fbe1-4a35-ae9e-611b3492afb6)
"
---
This is my log of the hour long lecture from Katrina Lawrence, an Applied Mathematician on ML mathematics.

The ganeda :
Derivates
Partia derivaticves
graadients
cartesian vectors
distance metrics (used in embedding spaces) 2-Norm and Cosine
eigen values

The lecture started off with derivatives:
Derivatives:
rules to be covered
power 
product 
quotient
chain rule


single variable calculus, with x as an input, Now for multiple variable calculus f(x,y), f(x,y,z,w).. we are not constrained by dimeniosn over three in mathematics.
For multiple variables, we use partial derivatives,

We then moved to the Gradients section.
ggardient is a colelction of partial derivaatives and the gradient is in itself a vector

we then moved to vectors
and how to find distance between them using eiher euclidean or Cosine methodology.

ANd then we finally solved for eigen values and eigen vectors.


Now for the second seeiosn today : Introduction to embeddings and retreival I will talk about this seperately.
